import streamlit as st
import pandas as pd
import yfinance as yf
from supabase import create_client
import os
from datetime import datetime, timedelta
import time
import requests
import ssl
from requests.packages.urllib3.exceptions import InsecureRequestWarning

# å¿½ç•¥ SSL è­¦å‘Š
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)

# --- 1. åˆå§‹åŒ–è¨­å®š ---
st.set_page_config(page_title="å¯Œé‚¦ D&O è£œæ¼æ¡é›†å™¨ (V10.1)", layout="wide", page_icon="ğŸ›¡ï¸")
st.title("ğŸ›¡ï¸ D&O æ™ºèƒ½æ ¸ä¿ - ç¼ºæ¼è³‡æ–™è£œè¶³ç³»çµ± (é˜²ç«ç‰†çªç ´ç‰ˆ)")

# è®€å– Supabase è¨­å®š
SUPABASE_URL = os.getenv("SUPABASE_URL") or st.secrets.get("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY") or st.secrets.get("SUPABASE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    st.error("âš ï¸ è«‹è¨­å®š Supabase URL èˆ‡ Key")
    st.stop()

supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

# --- 2. æ ¸å¿ƒåŠŸèƒ½ï¼šæŠ“å–å¸‚å ´ç¸½è¡¨ (ğŸ”¥ä¿®å¾© ConnectionResetError) ---
@st.cache_data(ttl=3600)
def get_all_tw_companies():
    """å¾è­‰äº¤æ‰€æŠ“å–ä¸¦åˆä½µæ¸…å–® (åŠ å…¥ç€è¦½å™¨å½è£)"""
    sources = [
        ("ä¸Šå¸‚", "https://isin.twse.com.tw/isin/C_public.jsp?strMode=2"),
        ("ä¸Šæ«ƒ", "https://isin.twse.com.tw/isin/C_public.jsp?strMode=4"),
        ("èˆˆæ«ƒ", "https://isin.twse.com.tw/isin/C_public.jsp?strMode=5")
    ]
    all_dfs = []
    
    # å»ºç«‹ä¸€å€‹ Session ä¸¦è¨­å®šå½è£ Header
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7'
    })

    try:
        for market_name, url in sources:
            # åŠ å…¥ timeout èˆ‡ verify=False
            response = session.get(url, verify=False, timeout=15)
            response.encoding = 'cp950'
            
            dfs = pd.read_html(response.text)
            df = dfs[0]
            
            df.columns = df.iloc[0]
            df = df.iloc[1:]
            df = df[df['æœ‰åƒ¹è­‰åˆ¸ä»£è™ŸåŠåç¨±'].notna()]
            df_stock = df[df['æœ‰åƒ¹è­‰åˆ¸ä»£è™ŸåŠåç¨±'].str.contains('ã€€')]
            df_stock[['ä»£è™Ÿ', 'åç¨±']] = df_stock['æœ‰åƒ¹è­‰åˆ¸ä»£è™ŸåŠåç¨±'].str.split('ã€€', expand=True).iloc[:, :2]
            df_stock['å¸‚å ´åˆ¥'] = market_name
            
            target_cols = ['ä»£è™Ÿ', 'åç¨±', 'å¸‚å ´åˆ¥', 'ç”¢æ¥­åˆ¥', 'ä¸Šå¸‚æ—¥']
            for col in target_cols:
                if col not in df_stock.columns:
                    df_stock[col] = "-"
            
            clean_df = df_stock[target_cols]
            clean_df = clean_df[clean_df['ä»£è™Ÿ'].str.match(r'^\d{4}$')]
            all_dfs.append(clean_df)
            
            # ç¦®è²Œæ€§æš«åœï¼Œé¿å…è¢«é– IP
            time.sleep(1)
            
        if all_dfs:
            return pd.concat(all_dfs, ignore_index=True)
        return pd.DataFrame()
    except Exception as e:
        st.error(f"è®€å–æ¸…å–®å¤±æ•— (è«‹ç¨å¾Œå†è©¦): {e}")
        return pd.DataFrame()

def get_existing_codes():
    """å¾ Supabase åˆ†é å–å¾—æ‰€æœ‰å…¬å¸ä»£è™Ÿ (çªç ´ 1000 ç­†é™åˆ¶)"""
    try:
        all_codes = set()
        start = 0
        batch_size = 1000 
        
        while True:
            response = supabase.table("underwriting_cache").select("code").range(start, start + batch_size - 1).execute()
            data = response.data
            
            if not data:
                break
            for item in data:
                all_codes.add(str(item['code']))
            if len(data) < batch_size:
                break
            start += batch_size
            
        return all_codes
    except Exception as e:
        st.error(f"è®€å–è³‡æ–™åº«å¤±æ•—: {e}")
        return set()

# --- 3. è¼”åŠ©å‡½æ•¸ ---
def date_to_roc_quarter(date_obj):
    year_roc = date_obj.year - 1911
    quarter = (date_obj.month - 1) // 3 + 1
    return f"{year_roc}å¹´ Q{quarter}"

def date_to_roc_year(date_obj):
    year_roc = date_obj.year - 1911
    return f"{year_roc}å¹´"

# --- ğŸ”¥ FinMind æ•‘æ´æŠ•æ‰‹ (V10 æ­·å²è¶¨å‹¢ç‰ˆ) ---
def fetch_finmind_data_history(stock_code):
    """
    ä½¿ç”¨ FinMind API V4 æŠ“å–æ­·å²è¶¨å‹¢æ•¸æ“š (è¿‘ 5 å­£)
    """
    try:
        start_date = (datetime.now() - timedelta(days=900)).strftime('%Y-%m-%d')
        base_url = "https://api.finmindtrade.com/api/v4/data"
        
        token = "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJkYXRlIjoiMjAyNi0wMi0wNiAxNDoxNToxMSIsInVzZXJfaWQiOiJqaW1teTk4NTA0MDIiLCJlbWFpbCI6IjExMDI1NTAyNEBnLm5jY3UuZWR1LnR3IiwiaXAiOiIyMjMuMTM3LjEwMC4xMjgifQ.2ou0rtCaMqV7XXPBh28jGWFJ7_4EQrtr2CdhNQ5YznI"
        headers = {"Authorization": f"Bearer {token}"}

        def get_fm_dataset(dataset_name):
            params = {
                "dataset": dataset_name,
                "data_id": stock_code,
                "start_date": start_date
            }
            try:
                res = requests.get(base_url, params=params, headers=headers, timeout=5)
                json_data = res.json()
                if json_data.get('msg') == 'success':
                    return json_data.get('data', [])
            except: pass
            return []

        data_income = get_fm_dataset("TaiwanStockFinancialStatements")
        data_balance = get_fm_dataset("TaiwanStockBalanceSheet")
        data_cash = get_fm_dataset("TaiwanStockCashFlowsStatement")
        data_rev = get_fm_dataset("TaiwanStockMonthRevenue")

        if not any([data_income, data_balance, data_cash, data_rev]):
            return None

        parsed_data = {
            "ç‡Ÿæ¥­æ”¶å…¥": {}, 
            "æ¯è‚¡ç›ˆé¤˜(EPS)": {}, 
            "ç¸½è³‡ç”¢": {}, 
            "ç¸½è² å‚µ": {},
            "æµå‹•è³‡ç”¢": {}, 
            "æµå‹•è² å‚µ": {},
            "è² å‚µæ¯”": {}, 
            "ç‡Ÿæ¥­æ´»å‹•æ·¨ç¾é‡‘æµ": {}
        }

        # --- 1. EPS ---
        if data_income:
            rows = [x for x in data_income if x['type'] in ['EPS', 'BasicEarningsPerShare']]
            rows.sort(key=lambda x: x['date'], reverse=True)
            for row in rows[:6]:
                q_key = date_to_roc_quarter(datetime.strptime(row['date'], '%Y-%m-%d'))
                parsed_data["æ¯è‚¡ç›ˆé¤˜(EPS)"][q_key] = f"{row['value']:.2f}"

        # --- 2. è³‡ç”¢è² å‚µ ---
        if data_balance:
            assets = {} 
            liabs = {}
            cur_assets = {}
            cur_liabs = {}
            
            for row in data_balance:
                d = row['date']
                v = row['value']
                t = row['type']
                if t == 'TotalAssets': assets[d] = v
                elif t in ['TotalLiabilities', 'Liabilities']: liabs[d] = v
                elif t in ['CurrentAssets']: cur_assets[d] = v
                elif t in ['CurrentLiabilities', 'LiabilitiesCurrent']: cur_liabs[d] = v
            
            sorted_dates = sorted(assets.keys(), reverse=True)[:6]
            for d in sorted_dates:
                q_key = date_to_roc_quarter(datetime.strptime(d, '%Y-%m-%d'))
                
                parsed_data["ç¸½è³‡ç”¢"][q_key] = f"{int(assets[d]/1000):,}"
                
                if d in liabs:
                    l_val = liabs[d]
                    parsed_data["ç¸½è² å‚µ"][q_key] = f"{int(l_val/1000):,}"
                    if assets[d] > 0:
                        ratio = (l_val / assets[d]) * 100
                        parsed_data["è² å‚µæ¯”"][q_key] = f"{ratio:.2f}%"
                
                if d in cur_assets: parsed_data["æµå‹•è³‡ç”¢"][q_key] = f"{int(cur_assets[d]/1000):,}"
                if d in cur_liabs: parsed_data["æµå‹•è² å‚µ"][q_key] = f"{int(cur_liabs[d]/1000):,}"

        # --- 3. ç¾é‡‘æµ ---
        if data_cash:
            targets = ['CashFlowFromOperatingActivities', 'CashFlowsFromOperatingActivities', 
                       'NetCashFlowsFromUsedInOperatingActivities', 'NetCashInflowFromOperatingActivities']
            rows = [x for x in data_cash if x['type'] in targets]
            rows.sort(key=lambda x: x['date'], reverse=True)
            for row in rows[:6]:
                q_key = date_to_roc_quarter(datetime.strptime(row['date'], '%Y-%m-%d'))
                parsed_data["ç‡Ÿæ¥­æ´»å‹•æ·¨ç¾é‡‘æµ"][q_key] = f"{int(row['value']/1000):,}"

        # --- 4. ç‡Ÿæ”¶ ---
        if data_rev:
            rows = sorted(data_rev, key=lambda x: x['date'], reverse=True)
            for row in rows[:8]:
                m_key = f"{row['date'][:7]} (æœˆ)"
                parsed_data["ç‡Ÿæ¥­æ”¶å…¥"][m_key] = f"{int(row['revenue']/1000):,}"

        # --- 5. æ ¼å¼åŒ–è¼¸å‡º ---
        formatted_list = []
        order = ["ç‡Ÿæ¥­æ”¶å…¥", "ç¸½è³‡ç”¢", "ç¸½è² å‚µ", "è² å‚µæ¯”", "æµå‹•è³‡ç”¢", "æµå‹•è² å‚µ", "æ¯è‚¡ç›ˆé¤˜(EPS)", "ç‡Ÿæ¥­æ´»å‹•æ·¨ç¾é‡‘æµ"]
        
        for item_name in order:
            if parsed_data[item_name]:
                row_dict = {"é …ç›®": item_name}
                row_dict.update(parsed_data[item_name])
                formatted_list.append(row_dict)
            else:
                formatted_list.append({"é …ç›®": item_name})

        formatted_list.append({"é …ç›®": "è³‡æ–™ä¾†æº", "èªªæ˜": "FinMind (èˆˆæ«ƒå‚™æ´)"})
        return formatted_list

    except Exception as e:
        print(f"FinMind History Error: {e}")
        return None

# --- 4. æ ¸å¿ƒçˆ¬èŸ² ---
def fetch_and_upload_data(stock_code, stock_name_tw=None, market_type="ä¸Šå¸‚"):
    suffix = ".TWO" if market_type in ["ä¸Šæ«ƒ", "èˆˆæ«ƒ"] else ".TW"
    ticker_symbol = f"{stock_code}{suffix}"
    stock = yf.Ticker(ticker_symbol)
    
    formatted_data = []
    source_used = "yfinance"

    try:
        # 1. å˜—è©¦ yfinance
        q_bs = stock.quarterly_balance_sheet
        q_is = stock.quarterly_financials
        
        if q_bs.empty or q_is.empty:
            # yfinance å¤±æ•— -> å•Ÿå‹• FinMind V10 (æ­·å²ç‰ˆ)
            fm_data_list = fetch_finmind_data_history(stock_code)
            
            if fm_data_list:
                source_used = "FinMind"
                formatted_data = fm_data_list
            else:
                return False, f"âŒ ç„¡æ•¸æ“šè·³é: {stock_name_tw}"
        else:
            # 2. yfinance æˆåŠŸ
            q_cf = stock.quarterly_cashflow 
            df_q = pd.concat([q_is.T, q_bs.T, q_cf.T], axis=1)
            df_q = df_q.loc[:, ~df_q.columns.duplicated()]
            df_q.index = pd.to_datetime(df_q.index)
            df_q_sorted = df_q.sort_index(ascending=False).head(12)

            a_bs = stock.balance_sheet
            a_is = stock.financials
            a_cf = stock.cashflow
            df_a_sorted = pd.DataFrame()
            if not a_is.empty:
                df_a = pd.concat([a_is.T, a_bs.T, a_cf.T], axis=1)
                df_a = df_a.loc[:, ~df_a.columns.duplicated()]
                df_a.index = pd.to_datetime(df_a.index)
                df_a_sorted = df_a.sort_index(ascending=False).head(5)

            mapping = {
                "Total Revenue": "ç‡Ÿæ¥­æ”¶å…¥", "Operating Revenue": "ç‡Ÿæ¥­æ”¶å…¥",
                "Total Assets": "ç¸½è³‡ç”¢",
                "Total Liabilities Net Minority Interest": "ç¸½è² å‚µ", "Total Liabilities": "ç¸½è² å‚µ",
                "Current Assets": "æµå‹•è³‡ç”¢", "Current Liabilities": "æµå‹•è² å‚µ",
                "Basic EPS": "æ¯è‚¡ç›ˆé¤˜(EPS)",
                "Operating Cash Flow": "ç‡Ÿæ¥­æ´»å‹•æ·¨ç¾é‡‘æµ", 
                "Total Cash From Operating Activities": "ç‡Ÿæ¥­æ´»å‹•æ·¨ç¾é‡‘æµ", 
                "Cash Flow From Continuing Operating Activities": "ç‡Ÿæ¥­æ´»å‹•æ·¨ç¾é‡‘æµ"
            }
            
            target_items = ["ç‡Ÿæ¥­æ”¶å…¥", "ç¸½è³‡ç”¢", "è² å‚µæ¯”", "æµå‹•è³‡ç”¢", "æµå‹•è² å‚µ", "æ¯è‚¡ç›ˆé¤˜(EPS)", "ç‡Ÿæ¥­æ´»å‹•æ·¨ç¾é‡‘æµ"]

            for target_name in target_items:
                row_dict = {"é …ç›®": target_name}
                for date_idx in df_q_sorted.index:
                    key_name = date_to_roc_quarter(date_idx)
                    val = extract_value(df_q_sorted, date_idx, target_name, mapping)
                    row_dict[key_name] = val
                
                if not df_a_sorted.empty:
                    for date_idx in df_a_sorted.index:
                        key_name = date_to_roc_year(date_idx)
                        val = extract_value(df_a_sorted, date_idx, target_name, mapping)
                        row_dict[key_name] = val
                
                formatted_data.append(row_dict)

        # 3. ä¸Šå‚³ Supabase
        final_name = stock_name_tw if stock_name_tw else stock.info.get('longName', stock_code)
        payload = {
            "code": stock_code,
            "name": final_name,
            "financial_data": formatted_data,
            "updated_at": datetime.now().isoformat()
        }
        supabase.table("underwriting_cache").upsert(payload).execute()
        
        icon = "âœ…" if source_used == "yfinance" else "ğŸš‘"
        return True, f"{icon} æˆåŠŸåŒæ­¥: {final_name} ({source_used})"

    except Exception as e:
        return False, str(e)

# æ•¸å€¼æå–
def extract_value(df, date_idx, target_name, mapping):
    if target_name == "è² å‚µæ¯”":
        try:
            liab = df.loc[date_idx].get("Total Liabilities Net Minority Interest") or df.loc[date_idx].get("Total Liabilities")
            assets = df.loc[date_idx].get("Total Assets")
            if liab and assets: return f"{(liab / assets) * 100:.2f}%"
        except: pass
        return "-"
    else:
        found_val = None
        for eng_col, ch_col in mapping.items():
            if ch_col == target_name and eng_col in df.columns:
                val = df.loc[date_idx, eng_col]
                if pd.notna(val): found_val = val; break
        
        if found_val is not None:
            if target_name != "æ¯è‚¡ç›ˆé¤˜(EPS)":
                try: return f"{int(found_val / 1000):,}"
                except: return "-"
            else: return f"{found_val:.2f}"
    return "-"

# --- 5. UI ä»‹é¢ ---
tab1, tab2 = st.tabs(["ğŸ” è£œæ¼ç›£æ§ä¸­å¿ƒ", "ğŸ“ å–®ç­†æ‰‹å‹•"])

with tab1:
    st.markdown("### ğŸ“‰ ç¼ºæ¼åå–®è£œè¶³ç³»çµ± (V10.1)")
    
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ”„ 1. æƒæç¼ºæ¼åå–®", type="primary"):
            with st.spinner("æ­£åœ¨æ¯”å°ä¸­ (è®€å–æ¸…å–®å¯èƒ½éœ€è¦ 10-20 ç§’ï¼Œè«‹ç¨å€™)..."):
                full_df = get_all_tw_companies()
                db_codes = get_existing_codes() 
                
                if not full_df.empty:
                    full_df['code_str'] = full_df['ä»£è™Ÿ'].astype(str).str.strip()
                    missing_df = full_df[~full_df['code_str'].isin(db_codes)].copy()
                    
                    st.session_state.missing_df = missing_df
                    st.session_state.db_count = len(db_codes)
                    st.success(f"æƒæå®Œæˆï¼è³‡æ–™åº«ç¾æœ‰ {len(db_codes)} ç­†ï¼Œç™¼ç¾ {len(missing_df)} å®¶ç¼ºæ¼ã€‚")

    if 'missing_df' in st.session_state:
        m_df = st.session_state.missing_df
        st.metric("ç›®å‰è³‡æ–™åº«ç¸½æ•¸", f"{st.session_state.db_count} å®¶")
        st.metric("ç¼ºæ¼å®¶æ•¸", f"{len(m_df)} å®¶", delta=f"-{len(m_df)}", delta_color="inverse")
        
        if not m_df.empty:
            st.dataframe(m_df[['ä»£è™Ÿ', 'åç¨±', 'å¸‚å ´åˆ¥', 'ç”¢æ¥­åˆ¥']].head(100), hide_index=True)
            
            if st.button(f"ğŸš€ 2. ç«‹å³è£œè¶³ {len(m_df)} å®¶è³‡æ–™"):
                p_bar = st.progress(0)
                status = st.empty()
                success_cnt = 0
                skip_cnt = 0
                
                total = len(m_df)
                for i, row in enumerate(m_df.itertuples()):
                    code = getattr(row, 'ä»£è™Ÿ')
                    name = getattr(row, 'åç¨±')
                    mkt = getattr(row, 'å¸‚å ´åˆ¥')
                    
                    status.text(f"è™•ç†ä¸­ ({i+1}/{total}): {code} {name} ...")
                    
                    ok, msg = fetch_and_upload_data(code, name, mkt)
                    if ok: success_cnt += 1
                    else: skip_cnt += 1
                    
                    p_bar.progress((i+1)/total)
                    time.sleep(0.1) 
                
                st.success(f"ğŸ‰ ä»»å‹™çµæŸï¼æˆåŠŸè£œå…¥: {success_cnt} å®¶ï¼Œç„¡è³‡æ–™è·³é: {skip_cnt} å®¶")
        else:
            st.success("æ­å–œï¼ç›®å‰è³‡æ–™åº«å®Œæ•´ç„¡ç¼ºæ¼ã€‚")

with tab2:
    st.markdown("### ğŸ“ æ‰‹å‹•å–®ç­†æŸ¥è©¢")
    s_in = st.text_input("è¼¸å…¥è‚¡ç¥¨ä»£è™Ÿ", value="1269")
    m_type = st.radio("å¸‚å ´", ["ä¸Šå¸‚", "ä¸Šæ«ƒ/èˆˆæ«ƒ"], horizontal=True)
    if st.button("åŸ·è¡Œå–®ç­†æ¡é›†"):
        real_mkt = "ä¸Šå¸‚" if "ä¸Šå¸‚" in m_type else "ä¸Šæ«ƒ"
        with st.spinner(f"æ­£åœ¨æŠ“å– {s_in}..."):
            suc, msg = fetch_and_upload_data(s_in, market_type=real_mkt)
            if suc: st.success(msg)
            else: st.error(msg)
